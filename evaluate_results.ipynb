{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for evaluating results after running inpainting models\n",
    "The inpainting models tries to recreate the background  behind an object that we want to remove. To evaluate performance, we are using perceptual hasinh, structural similarity and peak signal noise ratio to compute how \"realistic\" the output image is. If the model is able to recreate the background in a very realistic way, it should score well in these metrics.\n",
    "Note that there are two ways of evaluating performance here: \n",
    "* For unlabeled data we are using perceptual hashing and hamming distance to compute how realistic the output image is compared to the input image\n",
    "* For labeled data we are using structural similarity and peak SNR to compare the output image to the \"true output\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import imagehash\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating performance on unlabeled data\n",
    "Unlabeled data means that the only images we have available is the input image (where some object is present), and the output image (where the inpainting model has tried to recreate the background behind the object). We have no knowledge of what the \"true background\" behind the object is. Therefore, we are evaluating performance through perceptual hashing. Perceptual hashing provides a fingerprint of the image content, and a smaller hamming distance between hashes of the input and output image indicates higher realism in the generated output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on unlabeled data is evaluated by comparing similarity between perceptual\n",
    "# hashes of the input and output images. Perceptual hashes provide a fingerprint of the image \n",
    "# content, and a smaller Hamming distance between the hashes of the input and output images indicates higher realism.\n",
    "def perceptual_hash(image):\n",
    "    return imagehash.average_hash(Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "def evaluate_unlabeled(input_image_path, output_image_path):\n",
    "    # Load images\n",
    "    input_image = cv2.imread(input_image_path)\n",
    "    output_image = cv2.imread(output_image_path)\n",
    "\n",
    "    # Calculate perceptual hashes\n",
    "    input_hash = perceptual_hash(input_image)\n",
    "    output_hash = perceptual_hash(output_image)\n",
    "    \n",
    "    # Calculate Hamming distance\n",
    "    hamming_distance = input_hash - output_hash  # Lower values indicate higher similarity\n",
    "    \n",
    "    return hamming_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating performance on labeled data\n",
    "Labeled data means that in addition to the input image and the generated output image, we have some \"true output\" image available. In our case, that means that we have some image available where we have physically removed the object that the inpainting models tries to remove. This means that we now know what the background behind the given object looks like. To evaluate perfomance, we are computing the structural similarity (SSIM) and peak signal to noise ratio (PSNR) between the generated output image and the true output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labeled(output_image_path, ground_truth_output_path):\n",
    "    output_image = cv2.imread(output_image_path)\n",
    "    ground_truth_image = cv2.imread(ground_truth_output_path)\n",
    "    # convert gt image to same size as output image\n",
    "    height,width,_ = output_image.shape\n",
    "    ground_truth_image = cv2.resize(ground_truth_image,(width,height))\n",
    "    # Convert images to grayscale\n",
    "    output_gray = cv2.cvtColor(output_image, cv2.COLOR_BGR2GRAY)\n",
    "    ground_truth_image_gray = cv2.cvtColor(ground_truth_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate SSIM and PSNR\n",
    "    ssim_score = ssim(ground_truth_image_gray, output_gray)\n",
    "    psnr_score = psnr(ground_truth_image_gray, output_gray)\n",
    "    \n",
    "    return ssim_score, psnr_score\n",
    "\n",
    "def compute_mse(output_image_path, gt_image_path,mask_path):\n",
    "    # Load the images\n",
    "    output_image = cv2.imread(output_image_path)\n",
    "    gt_image = cv2.imread(gt_image_path)\n",
    "    mask_image = cv2.imread(mask_path)\n",
    "    # convert gt image to same size as output image\n",
    "    height,width,_ = output_image.shape\n",
    "    gt_image = cv2.resize(gt_image,(width,height))\n",
    "    #mask_image = cv2.resize(mask_image,(width,height))\n",
    "    # apply mask to output and gt\n",
    "    output_image = output_image[:, :, :3] * (mask_image[:, :, 3:] / 255.0)\n",
    "    gt_image = gt_image[:, :, :3] * (mask_image[:, :, 3:] / 255.0)\n",
    "   \n",
    "\n",
    "    # Compute MSE\n",
    "    mse = np.mean((output_image - gt_image) ** 2)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "input_image_path=\"images/inputs/truck_input.png\"\n",
    "output_image_path=\"images/outputs/truck_output.png\"\n",
    "output_image_path2=\"images/outputs/truck_output_tight.png\"\n",
    "\n",
    "hamming1 = evaluate_unlabeled(input_image_path,output_image_path)\n",
    "hamming2 = evaluate_unlabeled(input_image_path,output_image_path2)\n",
    "hamming3 = evaluate_unlabeled(input_image_path,input_image_path)\n",
    "\n",
    "print(hamming1)\n",
    "print(hamming2)\n",
    "print(hamming3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (800,600,3) (806,605,0) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mcase_nr\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mcase_nr\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmask_type\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_gan_mask.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ssim_score, psnr_score = evaluate_labeled(output_path,ground_truth_path)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# hamming= evaluate_unlabeled(input_image_path,output_path)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# display(case_nr+\": SSIM: \"+str(ssim_score)+\", PSNR: \"+str(psnr_score)+\", Hamming: \"+str(hamming))\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m mse \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_mse\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m display(case_nr\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: MSE: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(mse))\n",
      "Cell \u001b[0;32mIn[23], line 27\u001b[0m, in \u001b[0;36mcompute_mse\u001b[0;34m(output_image_path, gt_image_path, mask_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m gt_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(gt_image,(width,height))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#mask_image = cv2.resize(mask_image,(width,height))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# apply mask to output and gt\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m output_image \u001b[38;5;241m=\u001b[39m \u001b[43moutput_image\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_image\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m gt_image \u001b[38;5;241m=\u001b[39m gt_image[:, :, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m*\u001b[39m (mask_image[:, :, \u001b[38;5;241m3\u001b[39m:] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Compute MSE\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (800,600,3) (806,605,0) "
     ]
    }
   ],
   "source": [
    "for case_nr in [\"case01\",\"case02\",\"case03\", \"case04\",\"case05\",\"case06\",\"case07\",\"case08\",\"case09\",\"case10\"]:\n",
    "    mask_type = \"wide\"\n",
    "    model_type=\"gan\" # sd\n",
    "\n",
    "    input_image_path = \"images/\"+case_nr+\"/\"+case_nr+\"_input.png\"\n",
    "    output_path = \"images/\"+case_nr+\"/\"+case_nr+\"_\"+mask_type+\"_\"+model_type+\"_output.png\"\n",
    "    ground_truth_path = \"images/\"+case_nr+\"/\"+case_nr+\"_gt.png\"\n",
    "    mask_path = \"images/\"+case_nr+\"/\"+case_nr+\"_\"+mask_type+\"_gan_mask.png\"\n",
    "\n",
    "    # ssim_score, psnr_score = evaluate_labeled(output_path,ground_truth_path)\n",
    "    # hamming= evaluate_unlabeled(input_image_path,output_path)\n",
    "    \n",
    "    # display(case_nr+\": SSIM: \"+str(ssim_score)+\", PSNR: \"+str(psnr_score)+\", Hamming: \"+str(hamming))\n",
    "    mse = compute_mse(output_path, ground_truth_path,mask_path)\n",
    "    display(case_nr+\": MSE: \"+str(mse))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
