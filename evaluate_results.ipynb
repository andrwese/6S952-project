{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for evaluating results after running inpainting models\n",
    "The inpainting models tries to recreate the background  behind an object that we want to remove. To evaluate performance, we are using perceptual hasinh, structural similarity and peak signal noise ratio to compute how \"realistic\" the output image is. If the model is able to recreate the background in a very realistic way, it should score well in these metrics.\n",
    "Note that there are two ways of evaluating performance here: \n",
    "* For unlabeled data we are using perceptual hashing and hamming distance to compute how realistic the output image is compared to the input image\n",
    "* For labeled data we are using structural similarity and peak SNR to compare the output image to the \"true output\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import imagehash\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating performance on unlabeled data\n",
    "Unlabeled data means that the only images we have available is the input image (where some object is present), and the output image (where the inpainting model has tried to recreate the background behind the object). We have no knowledge of what the \"true background\" behind the object is. Therefore, we are evaluating performance through perceptual hashing. Perceptual hashing provides a fingerprint of the image content, and a smaller hamming distance between hashes of the input and output image indicates higher realism in the generated output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on unlabeled data is evaluated by comparing similarity between perceptual\n",
    "# hashes of the input and output images. Perceptual hashes provide a fingerprint of the image \n",
    "# content, and a smaller Hamming distance between the hashes of the input and output images indicates higher realism.\n",
    "def perceptual_hash(image):\n",
    "    return imagehash.average_hash(Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)))\n",
    "\n",
    "def evaluate_unlabeled(input_image_path, output_image_path):\n",
    "    # Load images\n",
    "    input_image = cv2.imread(input_image_path)\n",
    "    output_image = cv2.imread(output_image_path)\n",
    "\n",
    "    # Calculate perceptual hashes\n",
    "    input_hash = perceptual_hash(input_image)\n",
    "    output_hash = perceptual_hash(output_image)\n",
    "    \n",
    "    # Calculate Hamming distance\n",
    "    hamming_distance = input_hash - output_hash  # Lower values indicate higher similarity\n",
    "    \n",
    "    return hamming_distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating performance on labeled data\n",
    "Labeled data means that in addition to the input image and the generated output image, we have some \"true output\" image available. In our case, that means that we have some image available where we have physically removed the object that the inpainting models tries to remove. This means that we now know what the background behind the given object looks like. To evaluate perfomance, we are computing the structural similarity (SSIM) and peak signal to noise ratio (PSNR) between the generated output image and the true output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labeled(output_image, true_output_image):\n",
    "    # Convert images to grayscale\n",
    "    output_gray = cv2.cvtColor(output_image, cv2.COLOR_BGR2GRAY)\n",
    "    true_output_gray = cv2.cvtColor(true_output_image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate SSIM and PSNR\n",
    "    ssim_score = ssim(true_output_gray, output_gray)\n",
    "    psnr_score = psnr(true_output_gray, output_gray)\n",
    "    \n",
    "    return ssim_score, psnr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "13\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "input_image_path=\"images/inputs/truck_input.png\"\n",
    "output_image_path=\"images/outputs/truck_output.png\"\n",
    "output_image_path2=\"images/outputs/truck_output_tight.png\"\n",
    "\n",
    "hamming1 = evaluate_unlabeled(input_image_path,output_image_path)\n",
    "hamming2 = evaluate_unlabeled(input_image_path,output_image_path2)\n",
    "hamming3 = evaluate_unlabeled(input_image_path,input_image_path)\n",
    "\n",
    "print(hamming1)\n",
    "print(hamming2)\n",
    "print(hamming3)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
